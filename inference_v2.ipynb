{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4eb568c-71e0-4853-a047-0e045db16138",
   "metadata": {},
   "source": [
    "### Import and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a1ef16-c729-4f2a-8415-14084436f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "DEVICE = torch.device('cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "CHECKPOINT_PATH = '/Users/akshai/datasets/Truth Seeker/checkpoints/checkpoint_with_maxlength_400'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed47e8-0fad-42af-9280-5a036dc2e1b2",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5959d9bf-9793-4228-bd93-0806fd5c9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    CHECKPOINT_PATH, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "#Loading from statedict\n",
    "#model.load_state_dict(torch.load('final.ckpt'))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(DEVICE)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00b539-4ef4-44a8-9ffd-834958f2d4c4",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca5c9ae-e6a3-4ed3-a114-35fdde640df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATASET_PATH = \"/Users/akshai/datasets/Truth Seeker/TruthSeeker2023/Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "#Shuffling the dataset\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9f2e2a-9ab0-4075-8074-cf1533fc4c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']\n",
    "labels = df[\"BinaryNumTarget\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98622b2c-d561-4704-8e61-95ee1e79a478",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e76b5cc8-9d48-4dba-9776-ea14faff0fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "603it [00:00, 1523.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement: Elderly people were beat up by a BLM hate group for being white.| Tweet: @Colette_NYC Wow! I hope there's not a ton more videos that show the whole story. ANTIFA and BLM literally beat up an old man,  attacked elderly couples, multiple familes(a child was trampled) multiple arrests- look up the arrests and you tell me if you see them in prior Antifa riot videos. tensor(0., dtype=torch.float64)\n",
      "Statement: Biden campaign director arrested for electoral fraud.| Tweet: Twitter is trying to censor this-\n",
      "The Democrat Director of Texas state political strategy for the Joe Biden Presidential campaign has been arrested for electoral fraud..... tensor(0., dtype=torch.float64)\n",
      "Statement: This virus has a cure. It is called hydroxychloroquine, zinc, and Zithromax. I know you people want to talk about a mask. Hello? You dont need (a) mask. There is a cure.| Tweet: @laurenboebert Don't you want Americans to have to freedom to take Fentanyl or hydroxychloroquine or ivermectin to cure #COVID19?\n",
      "\n",
      "Why do you want government to ban drugs like ivermectin and Fentanyl and take away your freedom? tensor(0., dtype=torch.float64)\n",
      "Statement: Nancy Pelosi has invited all illegals to her mansion at 2724 Pacific Ave San Francisco CA.| Tweet: @Maureen03574575 @jgreen0807 @Davidga69239655 @FoxNews OH I AGREE!  Nancy Pelosi (D) hires illegals to work her vineyards, landscaping and in her mansion!  Wonder if she pays THEM minimum wage, payroll taxes, SS taxes, medical insurance, etc!  I DOUBT IT!  The Democrats LOVE their slaves!! They probably also played mommy and nanny! tensor(0., dtype=torch.float64)\n",
      "Statement: COVID-19 vaccinations are a violation of the Nuremberg code.\"| Tweet: @quippingalong @Lee351T @Glannie While the Federal mandate is over reach, one vaccine is fully approved. They are not experimental, nor is it a Nuremberg Code violation. I really wish people would stop lying about the vaccines as their way of fighting government over reach. It doesnt help the message. tensor(0., dtype=torch.float64)\n",
      "Statement: New evidence ties COVID-19 creation to research funded by Fauci\"| Tweet: @CP24 Fauci played a role in the creation of COVID-19. He should be fired and possibly imprisoned tensor(0., dtype=torch.float64)\n",
      "Statement: A quarter of a million illegal votes found in Arizonas audit.| Tweet: @MouseOfReason @DineshDSouza I can go on..Arizona audit proved fraud that actually gave Biden more votes in the count yet votes that cannot be verified as actually coming from a real person or real address.  So there is very much unconstitutional and illegal and suspect but no one really talks about it tensor(0., dtype=torch.float64)\n",
      "Statement: New Jersey has \"the highest property taxes in the nation and not by a little. They are the highest property taxes in the nation, more than double the national average.\"| Tweet: @charliekirk11 Yes! Like New Jersey with the highest property taxes. Liberals vote in crazy tax &amp; spend Dems. They complain about hi taxes &amp; cant afford to live there, so they go to N.Carolina. Theyre turning it more blue. Theyll ruin it then go to Tx &amp; ruin that. tensor(1., dtype=torch.float64)\n",
      "Statement: \"The majority of Austinites rent\" the places they live.| Tweet: .@bcabsalom No, I'm not dismissing renters. Austinites that rent and rely on public transit will benefit greatly from rail along Riverside. tensor(1., dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 1529.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "\n",
    "# For every sentence...\n",
    "for i, sent in tqdm(enumerate(sentences[:1000])):\n",
    "    if i > 300 and i < 310:\n",
    "        print (sent, labels[i])\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                     # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e91c61-eacc-4d34-8f28-248ba001af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217dbbde-6955-482d-858c-02af65b7bf7f",
   "metadata": {},
   "source": [
    "## Inference any single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "709c8cc7-c1dd-48db-9ce9-da4719f76f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410]) attention_mask.shape=torch.Size([1, 410])\n",
      "Prediction tensor(0) | Label: tensor([0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "INDEX = 235\n",
    "with torch.no_grad():\n",
    "    #output = model(dataset[INDEX:INDEX + 2][1], token_type_ids=None, attention_mask=dataset[INDEX:INDEX + 2][1],labels=None)\n",
    "    b_input_ids = dataset[INDEX:INDEX + 1][0]\n",
    "    attention_mask = dataset[INDEX:INDEX + 1][1]\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        print (f\"{b_input_ids.shape} {attention_mask.shape=}\")\n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=attention_mask,\n",
    "                               labels=None)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "    print (\"Prediction\", torch.argmax(logits), \"| Label:\", dataset[INDEX: INDEX+ 1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae1ddd-a0f7-4589-9c05-756c3e93e5dc",
   "metadata": {},
   "source": [
    "## Inference without the tensor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9598c058-b706-40ef-bf27-f004e35fe977",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_index = 150\n",
    "input_id_tensor = input_ids[inference_index]\n",
    "attention_mask_input_tensor = attention_masks[inference_index]\n",
    "output_label = labels[inference_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66b9110f-44f4-4daa-bf02-251402bc31b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([410])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71dbe09e-e778-4533-b77f-d971c2c4e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_tensor_expanded = input_id_tensor.unsqueeze(dim=0)\n",
    "attention_mask_input_tensor_expanded = attention_mask_input_tensor.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d85c28d-26ff-4a7a-ae67-ca98b1172dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 410]), torch.Size([1, 410]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id_tensor_expanded.shape, attention_mask_input_tensor_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65f3bdf7-8ca4-4ce6-87b7-079de195ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_id_tensor_expanded, token_type_ids=None,attention_mask=attention_mask_input_tensor_expanded,labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db0c3956-5004-4917-8cfa-6c5b917ad00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) tensor(1., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print (output.logits.argmax(), output_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b9880-9a26-42a2-8856-b54050dea596",
   "metadata": {},
   "source": [
    "## Inference on a new data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6c81f1d7-c3ea-43ba-8d80-9b74b33b4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 410\n",
    "test_sentence = 'Statement: \"The majority of Austinites rent\" the places they live.| Tweet: .@bcabsalom No, Im not dismissing renters. Austinites that rent and rely on public transit will benefit greatly from rail along Riverside.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "23b87875-7d98-4cae-913a-077716a71722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b58e0bb2-fe17-423f-9efa-4d57c2a86b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict_for_new_data_point = tokenizer.encode_plus(\n",
    "                        test_sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "# Add the encoded sentence to the list.    \n",
    "new_data_point_input_id = encoded_dict_for_new_data_point['input_ids']\n",
    "    \n",
    "# And its attention mask (simply differentiates padding from non-padding).\n",
    "new_data_point_attention_mask = encoded_dict_for_new_data_point['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7808105f-5181-44f4-8ac2-8a995d549cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "output = model(new_data_point_input_id, token_type_ids=None, attention_mask=new_data_point_attention_mask,labels=None)\n",
    "print (output.logits.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509be3f4-0bab-4d70-8cde-37dbaf892ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
