{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4eb568c-71e0-4853-a047-0e045db16138",
   "metadata": {},
   "source": [
    "### Import and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a1ef16-c729-4f2a-8415-14084436f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "DEVICE = torch.device('cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "CHECKPOINT_PATH = '/home/student/workspace/Truthseeker/Save_dir/bert'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed47e8-0fad-42af-9280-5a036dc2e1b2",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5959d9bf-9793-4228-bd93-0806fd5c9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    CHECKPOINT_PATH, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "#Loading from statedict\n",
    "#model.load_state_dict(torch.load('final.ckpt'))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(DEVICE)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00b539-4ef4-44a8-9ffd-834958f2d4c4",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca5c9ae-e6a3-4ed3-a114-35fdde640df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATASET_PATH = \"/home/student/workspace/Truthseeker/dataset/TruthSeeker2023/Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "#Shuffling the dataset\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9f2e2a-9ab0-4075-8074-cf1533fc4c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']\n",
    "labels = df[\"BinaryNumTarget\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98622b2c-d561-4704-8e61-95ee1e79a478",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e76b5cc8-9d48-4dba-9776-ea14faff0fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/student/miniconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "444it [00:00, 584.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement: Says Warren Buffett has publicly said his secretary \"should not be paying a higher tax rate\" than him.| Tweet: @man_integrated It is effectively progressive, and no one will be able to say \"Warren Buffet pays a lower tax rate than his secretary\" 1.0\n",
      "Statement: Says the 1986 Vaccine protection act resulted in the biggest jump in shots for children because it gave drug manufacturers exemption for responsibility of killing people.| Tweet: @CNN Next up has to be the - \n",
      "\n",
      "National Childhood Vaccine Injury Act of 1986 wrongly provides immunity to #bigpharma. \n",
      "\n",
      "What other industry/ company is free of liability  Name ONE! \n",
      "\n",
      "Thousands of children &amp; adults get injured every year &amp; we cant hold these companies accountable 0.0\n",
      "Statement: \"DMX received COVID vaccine days before heart attack.\"| Tweet: Dmx had a heart attack 2 days after taking the vaccine  family saying it had nothing to do with drugs . 0.0\n",
      "Statement: \"On Oct. 7, the Access Hollywood tape comes out. One hour later, WikiLeaks starts dropping my emails.\"| Tweet: @SenRonJohnson @JoeBiden How about the timing of the Wikileaks initial dump that came hot on the heels of the Access Hollywood tape coming out?  You know, the details of which are in the Senate Intel report that you pretend says \"no collusion\"?  Fucking traitor. 1.0\n",
      "Statement: Says income tax rates under Eisenhower were as high as 90 percent.| Tweet: @deonteleologist 2) The US labor movement, William Jennings Bryant, Suffrage movement, the income tax, Teddy Roosevelt, FDR (and Eleanor), the New Deal programs, Social Security, Truman, Eisenhower and his cabinet, etc., all labeled socialist/communist. 1.0\n",
      "Statement: Says Jacob Blake declared he had a gun in the car and was going to go get it and was brandishing a knife| Tweet: @JensenKneeland @KING5Seattle Jacob Blake is a violent sex offender who was holding a knife while resisting arrest and going for a gun in his car. Youre a moron. 0.0\n",
      "Statement: The movie I am Legend was based in the year 2021 and people turned into zombies because of the wrong vaccine.| Tweet: @AnthonyClarkAU Im pro vaccine, but this is also how the movie I am legend with will smith began. So let us know if you turn into a zombie! 0.0\n",
      "Statement: \"Did you know US population growth is at its lowest since the Great Depression?\"| Tweet: @Foote55 @dennisnahas @AnaKasparian @jdbuatti Pre-COVID LA had the second lowest rental vacancy rate in the context - 2.2%. LA also leads US in overcrowding, overpayment and homelessness. It isnt a coincidence. LA has built less housing than any other city in the US vs population growth since 80s 1.0\n",
      "Statement: \"Nearly 6 out of 10 believe that money and wealth should be more evenly distributed among a larger percentage of the people in the U.S.\"| Tweet: @calhistorian @Vlachbild @Jeanvaljean689 Of course. But it would be interesting to employ one of the largest motivators in the mankind: money. Of course, it could be more complicated, like that it would matter whether the participation is evenly distributed or not. 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:01, 568.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "\n",
    "# For every sentence...\n",
    "for i, sent in tqdm(enumerate(sentences[:1000])):\n",
    "    if i > 300 and i < 310:\n",
    "        print (sent, labels[i])\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                     # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e91c61-eacc-4d34-8f28-248ba001af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217dbbde-6955-482d-858c-02af65b7bf7f",
   "metadata": {},
   "source": [
    "## Inference any single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709c8cc7-c1dd-48db-9ce9-da4719f76f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410]) attention_mask.shape=torch.Size([1, 410])\n",
      "Prediction tensor(0) | Label: tensor([1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "INDEX = 236\n",
    "with torch.no_grad():\n",
    "    #output = model(dataset[INDEX:INDEX + 2][1], token_type_ids=None, attention_mask=dataset[INDEX:INDEX + 2][1],labels=None)\n",
    "    b_input_ids = dataset[INDEX:INDEX + 1][0]\n",
    "    attention_mask = dataset[INDEX:INDEX + 1][1]\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        print (f\"{b_input_ids.shape} {attention_mask.shape=}\")\n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=attention_mask,\n",
    "                               labels=None)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "    print (\"Prediction\", torch.argmax(logits), \"| Label:\", dataset[INDEX: INDEX+ 1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae1ddd-a0f7-4589-9c05-756c3e93e5dc",
   "metadata": {},
   "source": [
    "## Inference without the tensor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9598c058-b706-40ef-bf27-f004e35fe977",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_index = 150\n",
    "input_id_tensor = input_ids[inference_index]\n",
    "attention_mask_input_tensor = attention_masks[inference_index]\n",
    "output_label = labels[inference_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66b9110f-44f4-4daa-bf02-251402bc31b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([410])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71dbe09e-e778-4533-b77f-d971c2c4e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_tensor_expanded = input_id_tensor.unsqueeze(dim=0)\n",
    "attention_mask_input_tensor_expanded = attention_mask_input_tensor.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d85c28d-26ff-4a7a-ae67-ca98b1172dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 410]), torch.Size([1, 410]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id_tensor_expanded.shape, attention_mask_input_tensor_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65f3bdf7-8ca4-4ce6-87b7-079de195ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_id_tensor_expanded, token_type_ids=None,attention_mask=attention_mask_input_tensor_expanded,labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db0c3956-5004-4917-8cfa-6c5b917ad00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) tensor(1., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print (output.logits.argmax(), output_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b9880-9a26-42a2-8856-b54050dea596",
   "metadata": {},
   "source": [
    "## Inference on a new data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6c81f1d7-c3ea-43ba-8d80-9b74b33b4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 410\n",
    "test_sentence = 'Statement: \"The majority of Austinites rent\" the places they live.| Tweet: .@bcabsalom No, Im not dismissing renters. Austinites that rent and rely on public transit will benefit greatly from rail along Riverside.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "23b87875-7d98-4cae-913a-077716a71722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b58e0bb2-fe17-423f-9efa-4d57c2a86b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict_for_new_data_point = tokenizer.encode_plus(\n",
    "                        test_sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "# Add the encoded sentence to the list.    \n",
    "new_data_point_input_id = encoded_dict_for_new_data_point['input_ids']\n",
    "    \n",
    "# And its attention mask (simply differentiates padding from non-padding).\n",
    "new_data_point_attention_mask = encoded_dict_for_new_data_point['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7808105f-5181-44f4-8ac2-8a995d549cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "output = model(new_data_point_input_id, token_type_ids=None, attention_mask=new_data_point_attention_mask,labels=None)\n",
    "print (output.logits.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509be3f4-0bab-4d70-8cde-37dbaf892ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
