{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4eb568c-71e0-4853-a047-0e045db16138",
   "metadata": {},
   "source": [
    "### Import and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2a1ef16-c729-4f2a-8415-14084436f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "DEVICE = torch.device('cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "CHECKPOINT_PATH = '/home/student/workspace/Truthseeker/checkpoints/checkpoint_with_maxlength_400'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed47e8-0fad-42af-9280-5a036dc2e1b2",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5959d9bf-9793-4228-bd93-0806fd5c9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    CHECKPOINT_PATH, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "#Loading from statedict\n",
    "#model.load_state_dict(torch.load('final.ckpt'))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(DEVICE)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00b539-4ef4-44a8-9ffd-834958f2d4c4",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ca5c9ae-e6a3-4ed3-a114-35fdde640df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATASET_PATH = \"/home/student/datasets/TruthSeeker2023/Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "#Shuffling the dataset\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e9f2e2a-9ab0-4075-8074-cf1533fc4c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']\n",
    "labels = df[\"BinaryNumTarget\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98622b2c-d561-4704-8e61-95ee1e79a478",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e76b5cc8-9d48-4dba-9776-ea14faff0fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/1000 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/student/miniconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████| 1000/1000 [00:01<00:00, 622.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences[:1000]):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5e91c61-eacc-4d34-8f28-248ba001af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217dbbde-6955-482d-858c-02af65b7bf7f",
   "metadata": {},
   "source": [
    "## Inference any single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "709c8cc7-c1dd-48db-9ce9-da4719f76f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction tensor(0) | Label: tensor([0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "INDEX = 235\n",
    "with torch.no_grad():\n",
    "    #output = model(dataset[INDEX:INDEX + 2][1], token_type_ids=None, attention_mask=dataset[INDEX:INDEX + 2][1],labels=None)\n",
    "    b_input_ids = dataset[INDEX:INDEX + 1][0]\n",
    "    attention_mask = dataset[INDEX:INDEX + 1][1]\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        \n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=attention_mask,\n",
    "                               labels=None)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "    print (\"Prediction\", torch.argmax(logits), \"| Label:\", dataset[INDEX: INDEX+ 1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7ab16-022f-4169-af20-cd0106e28bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
