{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b56a26-dc6a-43bb-8398-a76752038969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = \"/Users/akshai/datasets/Truth Seeker/TruthSeeker2023/Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cffe6a-71d1-425e-9f78-a089d4e5e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13803549-d9c5-4d3f-8d3c-c496cba66ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: @POTUS Biden Blunders - 6 Month Update\\n\\nInflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8caac61c-165e-4325-83cd-08de139abaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"BinaryNumTarget\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01944a71-51db-47e2-b7f8-41fe07d1609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshai/anaconda3/envs/truth_seeker/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 4.82kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 6.96MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 6.69MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 189kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5031cf4e-3047-4bdd-99d7-90d1f2fb2d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: @POTUS Biden Blunders - 6 Month Update\n",
      "\n",
      "Inflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?\n",
      "Tokenized:  ['statement', ':', 'end', 'of', 'ev', '##iction', 'mora', '##torium', 'means', 'millions', 'of', 'americans', 'could', 'lose', 'their', 'housing', 'in', 'the', 'middle', 'of', 'a', 'pan', '##de', '##mic', '.', '|', 't', '##wee', '##t', ':', '@', 'pot', '##us', 'bid', '##en', 'blu', '##nder', '##s', '-', '6', 'month', 'update', 'inflation', ',', 'delta', 'mis', '##mana', '##gement', ',', 'co', '##vid', 'for', 'kids', ',', 'abandoning', 'americans', 'in', 'afghanistan', ',', 'arm', '##ing', 'the', 'taliban', ',', 's', '.', 'border', 'crisis', ',', 'breaking', 'job', 'growth', ',', 'abuse', 'of', 'power', '(', 'many', 'ex', '##ec', 'orders', ',', '$', '3', '.', '5', '##t', 'through', 'reconciliation', ',', 'ev', '##iction', 'mora', '##torium', ')', '.', '.', '.', 'what', 'did', 'i', 'miss', '?']\n",
      "Token IDs:  [4861, 1024, 2203, 1997, 23408, 28097, 26821, 24390, 2965, 8817, 1997, 4841, 2071, 4558, 2037, 3847, 1999, 1996, 2690, 1997, 1037, 6090, 3207, 7712, 1012, 1064, 1056, 28394, 2102, 1024, 1030, 8962, 2271, 7226, 2368, 14154, 11563, 2015, 1011, 1020, 3204, 10651, 14200, 1010, 7160, 28616, 24805, 20511, 1010, 2522, 17258, 2005, 4268, 1010, 19816, 4841, 1999, 7041, 1010, 2849, 2075, 1996, 16597, 1010, 1055, 1012, 3675, 5325, 1010, 4911, 3105, 3930, 1010, 6905, 1997, 2373, 1006, 2116, 4654, 8586, 4449, 1010, 1002, 1017, 1012, 1019, 2102, 2083, 16088, 1010, 23408, 28097, 26821, 24390, 1007, 1012, 1012, 1012, 2054, 2106, 1045, 3335, 1029]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea3b9c89-44de-4609-a518-3052a589908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████████▎                                                                      | 25162/134198 [00:14<01:01, 1765.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# For every sentence...\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m tqdm(sentences):\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Tokenize the text and add `[CLS]` and `[SEP]` tokens.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Update the maximum sentence length.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_len, \u001b[38;5;28mlen\u001b[39m(input_ids))\n",
      "File \u001b[0;32m~/anaconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2569\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2533\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2534\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2553\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2569\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2977\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2969\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2970\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2975\u001b[0m )\n\u001b[0;32m-> 2977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[0;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    739\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/tokenization_utils.py:686\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 686\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 617\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m~/anaconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py:245\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    243\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[1;32m    250\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m~/anaconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py:423\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# union() returns a new set by concatenating the two sets.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m never_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(never_split)) \u001b[38;5;28;01mif\u001b[39;00m never_split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\n\u001b[0;32m--> 423\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n",
      "File \u001b[0;32m~/anaconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py:530\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    528\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 530\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca91cb45-1857-4574-a619-8cbba56273d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 0/134198 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/student/miniconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████| 134198/134198 [03:51<00:00, 580.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dba6647-c6d0-4fe5-994b-661ab9b432cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ae78572-5c97-48b2-b8fd-1635b5745830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: @Riguy_453 @PaulSorrentino3 @POTUS Trump didn't attempt to mandate mask!! Biden wanted to more than the vaccine but you can't get Republican AMEricans to comply \n",
      "It was the Supreme Court that knocked the eviction moratorium on the head. The Amy Coney Barratts. Another Trump dirty trick.\n",
      "Token IDs: tensor([  101,  4861,  1024,  7226,  2368,  2165,  2091,  2852,  1012,  7367,\n",
      "        17854,  1998,  2720,  1012, 14557,  2132,  1012,  1064,  1056, 28394,\n",
      "         2102,  1024,  1030,  6943, 12171,  2099, 16147,  1030, 21721, 14045,\n",
      "         2869,  1030, 14580, 29451, 18301,  2001,  6719,  2091,  2000,  2322,\n",
      "         1003,  1997,  2049,  2434,  3700,  2043,  8398,  2165,  2436,  1998,\n",
      "         1996,  2510,  1005,  1055,  5656,  2000,  5383,  1998, 11027,  1996,\n",
      "         6893,  9601,  8134,  2053,  2689,  2013,  2054,  2001,  7462,  2077,\n",
      "         8398,  2165,  2436,  1012,  1998,  7226,  2368,  2018,  2498,  2000,\n",
      "         2079,  2007,  9790,  4757,  2030, 14557,  2132,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Labels: tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 15\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[index])\n",
    "print('Token IDs:', input_ids[index])\n",
    "print ('Labels:', labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5374dda6-4c51-4179-a733-e5889f8dd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134198, 410])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f2e335c-d319-42a0-a4f8-719cc21c29bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120,778 training samples\n",
      "13,420 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f06f5391-9385-4566-9200-92eebe31108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 2\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = RandomSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6bc13d4-68d2-481a-8431-21764fed64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "402ce2a1-a2bf-43af-86e8-322076484fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08ab44ef-2a35-4b1d-a9a0-83598f033094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/miniconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f73d228-43b8-4a2e-afc4-18df28d17523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "debfe942-a9b0-4990-88d6-50093725dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "427c284e-eb0e-4e17-a9d0-7f990c56176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46b26116-d8a9-482c-9867-8d1fab53a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49e6e9bf-6f44-439f-bc7a-97b017247a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = '/home/student/workspace/Truthseeker/checkpoints/checkpoint_with_maxlength_400'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c612aa1f-af4e-47df-88b9-97f64475d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  60,389.    Elapsed: 0:00:07. Training loss. 0.012186349369585514 Num fake examples 43 Num true examples 37\n",
      "  Batch    80  of  60,389.    Elapsed: 0:00:14. Training loss. 0.008228340186178684 Num fake examples 94 Num true examples 66\n",
      "  Batch   120  of  60,389.    Elapsed: 0:00:22. Training loss. 0.006724339909851551 Num fake examples 135 Num true examples 105\n",
      "  Batch   160  of  60,389.    Elapsed: 0:00:29. Training loss. 2.440878391265869 Num fake examples 176 Num true examples 144\n",
      "  Batch   200  of  60,389.    Elapsed: 0:00:36. Training loss. 0.019747167825698853 Num fake examples 215 Num true examples 185\n",
      "  Batch   240  of  60,389.    Elapsed: 0:00:43. Training loss. 0.007230784744024277 Num fake examples 256 Num true examples 224\n",
      "  Batch   280  of  60,389.    Elapsed: 0:00:50. Training loss. 0.004172320012003183 Num fake examples 304 Num true examples 256\n",
      "  Batch   320  of  60,389.    Elapsed: 0:00:58. Training loss. 0.003006668295711279 Num fake examples 352 Num true examples 288\n",
      "  Batch   360  of  60,389.    Elapsed: 0:01:05. Training loss. 0.0024607835803180933 Num fake examples 395 Num true examples 325\n",
      "  Batch   400  of  60,389.    Elapsed: 0:01:12. Training loss. 0.002581494627520442 Num fake examples 434 Num true examples 366\n",
      "  Batch   440  of  60,389.    Elapsed: 0:01:20. Training loss. 0.0031800817232578993 Num fake examples 471 Num true examples 409\n",
      "  Batch   480  of  60,389.    Elapsed: 0:01:27. Training loss. 0.0027542051393538713 Num fake examples 518 Num true examples 442\n",
      "  Batch   520  of  60,389.    Elapsed: 0:01:34. Training loss. 0.00395281333476305 Num fake examples 570 Num true examples 470\n",
      "  Batch   560  of  60,389.    Elapsed: 0:01:42. Training loss. 0.0021160156466066837 Num fake examples 606 Num true examples 514\n",
      "  Batch   600  of  60,389.    Elapsed: 0:01:49. Training loss. 0.0017929214518517256 Num fake examples 650 Num true examples 550\n",
      "  Batch   640  of  60,389.    Elapsed: 0:01:56. Training loss. 1.6314697265625 Num fake examples 689 Num true examples 591\n",
      "  Batch   680  of  60,389.    Elapsed: 0:02:04. Training loss. 0.001429339637979865 Num fake examples 730 Num true examples 630\n",
      "  Batch   720  of  60,389.    Elapsed: 0:02:11. Training loss. 0.001214067917317152 Num fake examples 771 Num true examples 669\n",
      "  Batch   760  of  60,389.    Elapsed: 0:02:19. Training loss. 0.003387011820450425 Num fake examples 815 Num true examples 705\n",
      "  Batch   800  of  60,389.    Elapsed: 0:02:26. Training loss. 0.001274946378543973 Num fake examples 857 Num true examples 743\n",
      "  Batch   840  of  60,389.    Elapsed: 0:02:33. Training loss. 0.0015885941684246063 Num fake examples 900 Num true examples 780\n",
      "  Batch   880  of  60,389.    Elapsed: 0:02:41. Training loss. 0.001639636466279626 Num fake examples 938 Num true examples 822\n",
      "  Batch   920  of  60,389.    Elapsed: 0:02:48. Training loss. 0.0012633735314011574 Num fake examples 970 Num true examples 870\n",
      "  Batch   960  of  60,389.    Elapsed: 0:02:55. Training loss. 0.0009086730424314737 Num fake examples 1010 Num true examples 910\n",
      "  Batch 1,000  of  60,389.    Elapsed: 0:03:03. Training loss. 0.0010918210027739406 Num fake examples 1055 Num true examples 945\n",
      "  Batch 1,040  of  60,389.    Elapsed: 0:03:10. Training loss. 0.0013114656321704388 Num fake examples 1098 Num true examples 982\n",
      "  Batch 1,080  of  60,389.    Elapsed: 0:03:17. Training loss. 0.0011896167416125536 Num fake examples 1142 Num true examples 1018\n",
      "  Batch 1,120  of  60,389.    Elapsed: 0:03:25. Training loss. 0.0011752446880564094 Num fake examples 1177 Num true examples 1063\n",
      "  Batch 1,160  of  60,389.    Elapsed: 0:03:32. Training loss. 0.0011869902955368161 Num fake examples 1217 Num true examples 1103\n",
      "  Batch 1,200  of  60,389.    Elapsed: 0:03:39. Training loss. 0.0010072445729747415 Num fake examples 1257 Num true examples 1143\n",
      "  Batch 1,240  of  60,389.    Elapsed: 0:03:47. Training loss. 0.001437061931937933 Num fake examples 1298 Num true examples 1182\n",
      "  Batch 1,280  of  60,389.    Elapsed: 0:03:54. Training loss. 0.0013547540875151753 Num fake examples 1339 Num true examples 1221\n",
      "  Batch 1,320  of  60,389.    Elapsed: 0:04:02. Training loss. 0.0010645815636962652 Num fake examples 1384 Num true examples 1256\n",
      "  Batch 1,360  of  60,389.    Elapsed: 0:04:09. Training loss. 0.0009823613800108433 Num fake examples 1420 Num true examples 1300\n",
      "  Batch 1,400  of  60,389.    Elapsed: 0:04:16. Training loss. 0.0007173748454079032 Num fake examples 1458 Num true examples 1342\n",
      "  Batch 1,440  of  60,389.    Elapsed: 0:04:24. Training loss. 0.000601526815444231 Num fake examples 1500 Num true examples 1380\n",
      "  Batch 1,480  of  60,389.    Elapsed: 0:04:31. Training loss. 0.0009353760397061706 Num fake examples 1538 Num true examples 1422\n",
      "  Batch 1,520  of  60,389.    Elapsed: 0:04:38. Training loss. 0.0006773810018785298 Num fake examples 1573 Num true examples 1467\n",
      "  Batch 1,560  of  60,389.    Elapsed: 0:04:46. Training loss. 0.004201230593025684 Num fake examples 1621 Num true examples 1499\n",
      "  Batch 1,600  of  60,389.    Elapsed: 0:04:53. Training loss. 0.006075895391404629 Num fake examples 1658 Num true examples 1542\n",
      "  Batch 1,640  of  60,389.    Elapsed: 0:05:00. Training loss. 0.0009049863438121974 Num fake examples 1706 Num true examples 1574\n",
      "  Batch 1,680  of  60,389.    Elapsed: 0:05:07. Training loss. 0.0007639141986146569 Num fake examples 1743 Num true examples 1617\n",
      "  Batch 1,720  of  60,389.    Elapsed: 0:05:15. Training loss. 0.0009937337599694729 Num fake examples 1781 Num true examples 1659\n",
      "  Batch 1,760  of  60,389.    Elapsed: 0:05:22. Training loss. 0.0005504090222530067 Num fake examples 1820 Num true examples 1700\n",
      "  Batch 1,800  of  60,389.    Elapsed: 0:05:29. Training loss. 0.001196055207401514 Num fake examples 1859 Num true examples 1741\n",
      "  Batch 1,840  of  60,389.    Elapsed: 0:05:37. Training loss. 0.0006356585072353482 Num fake examples 1901 Num true examples 1779\n",
      "  Batch 1,880  of  60,389.    Elapsed: 0:05:44. Training loss. 0.0006422409787774086 Num fake examples 1942 Num true examples 1818\n",
      "  Batch 1,920  of  60,389.    Elapsed: 0:05:51. Training loss. 0.0005344125092960894 Num fake examples 1981 Num true examples 1859\n",
      "  Batch 1,960  of  60,389.    Elapsed: 0:05:59. Training loss. 0.0005702562048099935 Num fake examples 2027 Num true examples 1893\n",
      "  Batch 2,000  of  60,389.    Elapsed: 0:06:06. Training loss. 0.0008822325034998357 Num fake examples 2069 Num true examples 1931\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:06:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.29\n",
      "  Validation Loss: 0.02\n",
      "  Validation took: 0:01:40\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  60,389.    Elapsed: 0:00:07. Training loss. 0.0005809136782772839 Num fake examples 43 Num true examples 37\n",
      "  Batch    80  of  60,389.    Elapsed: 0:00:15. Training loss. 0.00042917794780805707 Num fake examples 82 Num true examples 78\n",
      "  Batch   120  of  60,389.    Elapsed: 0:00:22. Training loss. 0.0007033329457044601 Num fake examples 125 Num true examples 115\n",
      "  Batch   160  of  60,389.    Elapsed: 0:00:29. Training loss. 0.2689252197742462 Num fake examples 170 Num true examples 150\n",
      "  Batch   200  of  60,389.    Elapsed: 0:00:37. Training loss. 0.000653343100566417 Num fake examples 204 Num true examples 196\n",
      "  Batch   240  of  60,389.    Elapsed: 0:00:44. Training loss. 0.0005646200734190643 Num fake examples 250 Num true examples 230\n",
      "  Batch   280  of  60,389.    Elapsed: 0:00:51. Training loss. 0.0010388314258307219 Num fake examples 301 Num true examples 259\n",
      "  Batch   320  of  60,389.    Elapsed: 0:00:59. Training loss. 0.0005534212687052786 Num fake examples 342 Num true examples 298\n",
      "  Batch   360  of  60,389.    Elapsed: 0:01:06. Training loss. 0.0005578004056587815 Num fake examples 372 Num true examples 348\n",
      "  Batch   400  of  60,389.    Elapsed: 0:01:13. Training loss. 0.0004570925375446677 Num fake examples 416 Num true examples 384\n",
      "  Batch   440  of  60,389.    Elapsed: 0:01:21. Training loss. 0.000576716149225831 Num fake examples 454 Num true examples 426\n",
      "  Batch   480  of  60,389.    Elapsed: 0:01:28. Training loss. 0.00040495535358786583 Num fake examples 491 Num true examples 469\n",
      "  Batch   520  of  60,389.    Elapsed: 0:01:35. Training loss. 0.0005451360484585166 Num fake examples 531 Num true examples 509\n",
      "  Batch   560  of  60,389.    Elapsed: 0:01:42. Training loss. 0.00041258620331063867 Num fake examples 573 Num true examples 547\n",
      "  Batch   600  of  60,389.    Elapsed: 0:01:50. Training loss. 0.0004427343956194818 Num fake examples 620 Num true examples 580\n",
      "  Batch   640  of  60,389.    Elapsed: 0:01:57. Training loss. 0.0005738806794397533 Num fake examples 658 Num true examples 622\n",
      "  Batch   680  of  60,389.    Elapsed: 0:02:04. Training loss. 0.0006674664909951389 Num fake examples 700 Num true examples 660\n",
      "  Batch   720  of  60,389.    Elapsed: 0:02:12. Training loss. 0.00044401458580978215 Num fake examples 736 Num true examples 704\n",
      "  Batch   760  of  60,389.    Elapsed: 0:02:19. Training loss. 0.00044383638305589557 Num fake examples 769 Num true examples 751\n",
      "  Batch   800  of  60,389.    Elapsed: 0:02:26. Training loss. 0.00037666011485271156 Num fake examples 814 Num true examples 786\n",
      "  Batch   840  of  60,389.    Elapsed: 0:02:34. Training loss. 0.0003537203883752227 Num fake examples 854 Num true examples 826\n",
      "  Batch   880  of  60,389.    Elapsed: 0:02:41. Training loss. 0.00031773094087839127 Num fake examples 893 Num true examples 867\n",
      "  Batch   920  of  60,389.    Elapsed: 0:02:48. Training loss. 0.0005118403933010995 Num fake examples 932 Num true examples 908\n",
      "  Batch   960  of  60,389.    Elapsed: 0:02:56. Training loss. 0.00033140595769509673 Num fake examples 970 Num true examples 950\n",
      "  Batch 1,000  of  60,389.    Elapsed: 0:03:03. Training loss. 0.00035678865970112383 Num fake examples 1009 Num true examples 991\n",
      "  Batch 1,040  of  60,389.    Elapsed: 0:03:10. Training loss. 0.0003257745411247015 Num fake examples 1050 Num true examples 1030\n",
      "  Batch 1,080  of  60,389.    Elapsed: 0:03:18. Training loss. 0.0005953568033874035 Num fake examples 1093 Num true examples 1067\n",
      "  Batch 1,120  of  60,389.    Elapsed: 0:03:25. Training loss. 0.0005014440976083279 Num fake examples 1140 Num true examples 1100\n",
      "  Batch 1,160  of  60,389.    Elapsed: 0:03:32. Training loss. 0.0003326279402244836 Num fake examples 1176 Num true examples 1144\n",
      "  Batch 1,200  of  60,389.    Elapsed: 0:03:39. Training loss. 0.0002761684590950608 Num fake examples 1216 Num true examples 1184\n",
      "  Batch 1,240  of  60,389.    Elapsed: 0:03:47. Training loss. 0.0003159126208629459 Num fake examples 1265 Num true examples 1215\n",
      "  Batch 1,280  of  60,389.    Elapsed: 0:03:54. Training loss. 0.0005360505310818553 Num fake examples 1308 Num true examples 1252\n",
      "  Batch 1,320  of  60,389.    Elapsed: 0:04:01. Training loss. 0.00048088040784932673 Num fake examples 1358 Num true examples 1282\n",
      "  Batch 1,360  of  60,389.    Elapsed: 0:04:09. Training loss. 0.0003008973435498774 Num fake examples 1397 Num true examples 1323\n",
      "  Batch 1,400  of  60,389.    Elapsed: 0:04:16. Training loss. 0.00025680166436359286 Num fake examples 1437 Num true examples 1363\n",
      "  Batch 1,440  of  60,389.    Elapsed: 0:04:23. Training loss. 0.00035205140011385083 Num fake examples 1475 Num true examples 1405\n",
      "  Batch 1,480  of  60,389.    Elapsed: 0:04:31. Training loss. 0.0004195249639451504 Num fake examples 1518 Num true examples 1442\n",
      "  Batch 1,520  of  60,389.    Elapsed: 0:04:38. Training loss. 0.0003253533213865012 Num fake examples 1550 Num true examples 1490\n",
      "  Batch 1,560  of  60,389.    Elapsed: 0:04:45. Training loss. 0.0003032477106899023 Num fake examples 1584 Num true examples 1536\n",
      "  Batch 1,600  of  60,389.    Elapsed: 0:04:53. Training loss. 0.00033664258080534637 Num fake examples 1630 Num true examples 1570\n",
      "  Batch 1,640  of  60,389.    Elapsed: 0:05:00. Training loss. 0.0002673192066140473 Num fake examples 1671 Num true examples 1609\n",
      "  Batch 1,680  of  60,389.    Elapsed: 0:05:07. Training loss. 0.00024720735382288694 Num fake examples 1716 Num true examples 1644\n",
      "  Batch 1,720  of  60,389.    Elapsed: 0:05:15. Training loss. 0.00032872147858142853 Num fake examples 1757 Num true examples 1683\n",
      "  Batch 1,760  of  60,389.    Elapsed: 0:05:22. Training loss. 0.0003206767141819 Num fake examples 1795 Num true examples 1725\n",
      "  Batch 1,800  of  60,389.    Elapsed: 0:05:29. Training loss. 0.00027321692323312163 Num fake examples 1836 Num true examples 1764\n",
      "  Batch 1,840  of  60,389.    Elapsed: 0:05:37. Training loss. 0.0004329278599470854 Num fake examples 1876 Num true examples 1804\n",
      "  Batch 1,880  of  60,389.    Elapsed: 0:05:44. Training loss. 0.000883250031620264 Num fake examples 1911 Num true examples 1849\n",
      "  Batch 1,920  of  60,389.    Elapsed: 0:05:52. Training loss. 0.00034895315184257925 Num fake examples 1945 Num true examples 1895\n",
      "  Batch 1,960  of  60,389.    Elapsed: 0:05:59. Training loss. 0.00025998998899012804 Num fake examples 1989 Num true examples 1931\n",
      "  Batch 2,000  of  60,389.    Elapsed: 0:06:06. Training loss. 0.00043307963642291725 Num fake examples 2031 Num true examples 1969\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:06:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.30\n",
      "  Validation Loss: 0.01\n",
      "  Validation took: 0:01:40\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  60,389.    Elapsed: 0:00:07. Training loss. 0.00030730312573723495 Num fake examples 44 Num true examples 36\n",
      "  Batch    80  of  60,389.    Elapsed: 0:00:15. Training loss. 0.00027307093841955066 Num fake examples 84 Num true examples 76\n",
      "  Batch   120  of  60,389.    Elapsed: 0:00:22. Training loss. 0.0002888019662350416 Num fake examples 122 Num true examples 118\n",
      "  Batch   160  of  60,389.    Elapsed: 0:00:29. Training loss. 0.00018505516345612705 Num fake examples 165 Num true examples 155\n",
      "  Batch   200  of  60,389.    Elapsed: 0:00:36. Training loss. 0.014790826477110386 Num fake examples 203 Num true examples 197\n",
      "  Batch   240  of  60,389.    Elapsed: 0:00:44. Training loss. 0.0002847483556251973 Num fake examples 235 Num true examples 245\n",
      "  Batch   280  of  60,389.    Elapsed: 0:00:51. Training loss. 0.00018046653713099658 Num fake examples 286 Num true examples 274\n",
      "  Batch   320  of  60,389.    Elapsed: 0:00:58. Training loss. 0.00021017352992203087 Num fake examples 323 Num true examples 317\n",
      "  Batch   360  of  60,389.    Elapsed: 0:01:06. Training loss. 0.0001865448575699702 Num fake examples 366 Num true examples 354\n",
      "  Batch   400  of  60,389.    Elapsed: 0:01:13. Training loss. 0.00019312958465889096 Num fake examples 399 Num true examples 401\n",
      "  Batch   440  of  60,389.    Elapsed: 0:01:20. Training loss. 0.00014748077956028283 Num fake examples 443 Num true examples 437\n",
      "  Batch   480  of  60,389.    Elapsed: 0:01:27. Training loss. 0.00016088958363980055 Num fake examples 479 Num true examples 481\n",
      "  Batch   520  of  60,389.    Elapsed: 0:01:35. Training loss. 0.0001706331386230886 Num fake examples 518 Num true examples 522\n",
      "  Batch   560  of  60,389.    Elapsed: 0:01:42. Training loss. 0.00017608610505703837 Num fake examples 553 Num true examples 567\n",
      "  Batch   600  of  60,389.    Elapsed: 0:01:49. Training loss. 0.0002811120357364416 Num fake examples 599 Num true examples 601\n",
      "  Batch   640  of  60,389.    Elapsed: 0:01:56. Training loss. 0.000233086000662297 Num fake examples 641 Num true examples 639\n",
      "  Batch   680  of  60,389.    Elapsed: 0:02:04. Training loss. 0.00045521173160523176 Num fake examples 679 Num true examples 681\n",
      "  Batch   720  of  60,389.    Elapsed: 0:02:11. Training loss. 0.014974088408052921 Num fake examples 723 Num true examples 717\n",
      "  Batch   760  of  60,389.    Elapsed: 0:02:18. Training loss. 0.00020534664508886635 Num fake examples 762 Num true examples 758\n",
      "  Batch   800  of  60,389.    Elapsed: 0:02:26. Training loss. 0.00019464950310066342 Num fake examples 807 Num true examples 793\n",
      "  Batch   840  of  60,389.    Elapsed: 0:02:33. Training loss. 0.00021359907987061888 Num fake examples 847 Num true examples 833\n",
      "  Batch   880  of  60,389.    Elapsed: 0:02:40. Training loss. 0.00021800748072564602 Num fake examples 883 Num true examples 877\n",
      "  Batch   920  of  60,389.    Elapsed: 0:02:48. Training loss. 0.00016044281073845923 Num fake examples 929 Num true examples 911\n",
      "  Batch   960  of  60,389.    Elapsed: 0:02:55. Training loss. 0.00019521499052643776 Num fake examples 969 Num true examples 951\n",
      "  Batch 1,000  of  60,389.    Elapsed: 0:03:02. Training loss. 0.00041558893281035125 Num fake examples 1011 Num true examples 989\n",
      "  Batch 1,040  of  60,389.    Elapsed: 0:03:09. Training loss. 0.00015299281221814454 Num fake examples 1043 Num true examples 1037\n",
      "  Batch 1,080  of  60,389.    Elapsed: 0:03:17. Training loss. 0.00018192644347436726 Num fake examples 1076 Num true examples 1084\n",
      "  Batch 1,120  of  60,389.    Elapsed: 0:03:24. Training loss. 0.00018448903574608266 Num fake examples 1115 Num true examples 1125\n",
      "  Batch 1,160  of  60,389.    Elapsed: 0:03:31. Training loss. 0.00019217582303099334 Num fake examples 1154 Num true examples 1166\n",
      "  Batch 1,200  of  60,389.    Elapsed: 0:03:39. Training loss. 0.0001714677200652659 Num fake examples 1195 Num true examples 1205\n",
      "  Batch 1,240  of  60,389.    Elapsed: 0:03:46. Training loss. 0.0001415805018041283 Num fake examples 1229 Num true examples 1251\n",
      "  Batch 1,280  of  60,389.    Elapsed: 0:03:53. Training loss. 0.00046699587255716324 Num fake examples 1269 Num true examples 1291\n",
      "  Batch 1,320  of  60,389.    Elapsed: 0:04:00. Training loss. 0.0002739646297413856 Num fake examples 1307 Num true examples 1333\n",
      "  Batch 1,360  of  60,389.    Elapsed: 0:04:08. Training loss. 0.00023079208040144295 Num fake examples 1337 Num true examples 1383\n",
      "  Batch 1,400  of  60,389.    Elapsed: 0:04:15. Training loss. 0.00020343966025393456 Num fake examples 1375 Num true examples 1425\n",
      "  Batch 1,440  of  60,389.    Elapsed: 0:04:22. Training loss. 0.0001945897238329053 Num fake examples 1411 Num true examples 1469\n",
      "  Batch 1,480  of  60,389.    Elapsed: 0:04:30. Training loss. 0.00016678946849424392 Num fake examples 1452 Num true examples 1508\n",
      "  Batch 1,520  of  60,389.    Elapsed: 0:04:37. Training loss. 0.00016669991600792855 Num fake examples 1488 Num true examples 1552\n",
      "  Batch 1,560  of  60,389.    Elapsed: 0:04:44. Training loss. 0.0005840598605573177 Num fake examples 1536 Num true examples 1584\n",
      "  Batch 1,600  of  60,389.    Elapsed: 0:04:52. Training loss. 0.00028671586187556386 Num fake examples 1580 Num true examples 1620\n",
      "  Batch 1,640  of  60,389.    Elapsed: 0:04:59. Training loss. 0.0008181955199688673 Num fake examples 1622 Num true examples 1658\n",
      "  Batch 1,680  of  60,389.    Elapsed: 0:05:06. Training loss. 0.00023365262313745916 Num fake examples 1660 Num true examples 1700\n",
      "  Batch 1,720  of  60,389.    Elapsed: 0:05:13. Training loss. 0.0001983741531148553 Num fake examples 1712 Num true examples 1728\n",
      "  Batch 1,760  of  60,389.    Elapsed: 0:05:21. Training loss. 0.000620066246483475 Num fake examples 1752 Num true examples 1768\n",
      "  Batch 1,800  of  60,389.    Elapsed: 0:05:28. Training loss. 0.0003333422646392137 Num fake examples 1787 Num true examples 1813\n",
      "  Batch 1,840  of  60,389.    Elapsed: 0:05:35. Training loss. 0.00029341992922127247 Num fake examples 1829 Num true examples 1851\n",
      "  Batch 1,880  of  60,389.    Elapsed: 0:05:43. Training loss. 0.00043307902524247766 Num fake examples 1868 Num true examples 1892\n",
      "  Batch 1,920  of  60,389.    Elapsed: 0:05:50. Training loss. 0.0003003007441293448 Num fake examples 1915 Num true examples 1925\n",
      "  Batch 1,960  of  60,389.    Elapsed: 0:05:57. Training loss. 0.00026922632241621614 Num fake examples 1957 Num true examples 1963\n",
      "  Batch 2,000  of  60,389.    Elapsed: 0:06:05. Training loss. 0.00025581952650099993 Num fake examples 2006 Num true examples 1994\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:06:05\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.30\n",
      "  Validation Loss: 0.01\n",
      "  Validation took: 0:01:40\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  60,389.    Elapsed: 0:00:07. Training loss. 0.00023946218425408006 Num fake examples 44 Num true examples 36\n",
      "  Batch    80  of  60,389.    Elapsed: 0:00:14. Training loss. 0.0001978669606614858 Num fake examples 86 Num true examples 74\n",
      "  Batch   120  of  60,389.    Elapsed: 0:00:22. Training loss. 0.00015043064195197076 Num fake examples 129 Num true examples 111\n",
      "  Batch   160  of  60,389.    Elapsed: 0:00:29. Training loss. 0.0001942912640515715 Num fake examples 169 Num true examples 151\n",
      "  Batch   200  of  60,389.    Elapsed: 0:00:36. Training loss. 0.00025218469090759754 Num fake examples 205 Num true examples 195\n",
      "  Batch   240  of  60,389.    Elapsed: 0:00:44. Training loss. 0.00024011748610064387 Num fake examples 242 Num true examples 238\n",
      "  Batch   280  of  60,389.    Elapsed: 0:00:51. Training loss. 0.0002313574805157259 Num fake examples 282 Num true examples 278\n",
      "  Batch   320  of  60,389.    Elapsed: 0:00:58. Training loss. 0.0005262586055323482 Num fake examples 329 Num true examples 311\n",
      "  Batch   360  of  60,389.    Elapsed: 0:01:05. Training loss. 0.0008689294918440282 Num fake examples 377 Num true examples 343\n",
      "  Batch   400  of  60,389.    Elapsed: 0:01:13. Training loss. 0.0003050094819627702 Num fake examples 415 Num true examples 385\n",
      "  Batch   440  of  60,389.    Elapsed: 0:01:20. Training loss. 0.0007872082642279565 Num fake examples 459 Num true examples 421\n",
      "  Batch   480  of  60,389.    Elapsed: 0:01:27. Training loss. 0.00046036732965148985 Num fake examples 502 Num true examples 458\n",
      "  Batch   520  of  60,389.    Elapsed: 0:01:35. Training loss. 0.00045432025217451155 Num fake examples 547 Num true examples 493\n",
      "  Batch   560  of  60,389.    Elapsed: 0:01:42. Training loss. 0.0005372833111323416 Num fake examples 594 Num true examples 526\n",
      "  Batch   600  of  60,389.    Elapsed: 0:01:49. Training loss. 0.00030781025998294353 Num fake examples 631 Num true examples 569\n",
      "  Batch   640  of  60,389.    Elapsed: 0:01:56. Training loss. 0.00033280192292295396 Num fake examples 674 Num true examples 606\n",
      "  Batch   680  of  60,389.    Elapsed: 0:02:04. Training loss. 0.00025012894184328616 Num fake examples 713 Num true examples 647\n",
      "  Batch   720  of  60,389.    Elapsed: 0:02:11. Training loss. 0.0002315967285539955 Num fake examples 760 Num true examples 680\n",
      "  Batch   760  of  60,389.    Elapsed: 0:02:18. Training loss. 0.00020817722543142736 Num fake examples 801 Num true examples 719\n",
      "  Batch   800  of  60,389.    Elapsed: 0:02:26. Training loss. 0.00021848574397154152 Num fake examples 848 Num true examples 752\n",
      "  Batch   840  of  60,389.    Elapsed: 0:02:33. Training loss. 0.00017563934670761228 Num fake examples 892 Num true examples 788\n",
      "  Batch   880  of  60,389.    Elapsed: 0:02:40. Training loss. 0.00017879766528494656 Num fake examples 925 Num true examples 835\n",
      "  Batch   920  of  60,389.    Elapsed: 0:02:48. Training loss. 0.00030976926791481674 Num fake examples 974 Num true examples 866\n",
      "  Batch   960  of  60,389.    Elapsed: 0:02:55. Training loss. 0.00015525803610216826 Num fake examples 1021 Num true examples 899\n",
      "  Batch 1,000  of  60,389.    Elapsed: 0:03:02. Training loss. 0.00023618293926119804 Num fake examples 1065 Num true examples 935\n",
      "  Batch 1,040  of  60,389.    Elapsed: 0:03:10. Training loss. 0.00030563442851416767 Num fake examples 1095 Num true examples 985\n",
      "  Batch 1,080  of  60,389.    Elapsed: 0:03:17. Training loss. 0.0001746555499266833 Num fake examples 1132 Num true examples 1028\n",
      "  Batch 1,120  of  60,389.    Elapsed: 0:03:24. Training loss. 0.00013913714792579412 Num fake examples 1174 Num true examples 1066\n",
      "  Batch 1,160  of  60,389.    Elapsed: 0:03:31. Training loss. 0.0001568668958498165 Num fake examples 1212 Num true examples 1108\n",
      "  Batch 1,200  of  60,389.    Elapsed: 0:03:39. Training loss. 0.00015814817743375897 Num fake examples 1253 Num true examples 1147\n",
      "  Batch 1,240  of  60,389.    Elapsed: 0:03:46. Training loss. 0.0001127357900259085 Num fake examples 1288 Num true examples 1192\n",
      "  Batch 1,280  of  60,389.    Elapsed: 0:03:53. Training loss. 0.00010305098840035498 Num fake examples 1329 Num true examples 1231\n",
      "  Batch 1,320  of  60,389.    Elapsed: 0:04:00. Training loss. 0.0012653183657675982 Num fake examples 1365 Num true examples 1275\n",
      "  Batch 1,360  of  60,389.    Elapsed: 0:04:08. Training loss. 0.000211394508369267 Num fake examples 1408 Num true examples 1312\n",
      "  Batch 1,400  of  60,389.    Elapsed: 0:04:15. Training loss. 0.00015308268484659493 Num fake examples 1446 Num true examples 1354\n",
      "  Batch 1,440  of  60,389.    Elapsed: 0:04:22. Training loss. 0.00020272168330848217 Num fake examples 1485 Num true examples 1395\n",
      "  Batch 1,480  of  60,389.    Elapsed: 0:04:29. Training loss. 0.0003963283379562199 Num fake examples 1531 Num true examples 1429\n",
      "  Batch 1,520  of  60,389.    Elapsed: 0:04:37. Training loss. 0.00017382026999257505 Num fake examples 1577 Num true examples 1463\n",
      "  Batch 1,560  of  60,389.    Elapsed: 0:04:44. Training loss. 0.00014226564962882549 Num fake examples 1610 Num true examples 1510\n",
      "  Batch 1,600  of  60,389.    Elapsed: 0:04:51. Training loss. 0.00012009585771011189 Num fake examples 1647 Num true examples 1553\n",
      "  Batch 1,640  of  60,389.    Elapsed: 0:04:58. Training loss. 0.00011482171976240352 Num fake examples 1694 Num true examples 1586\n",
      "  Batch 1,680  of  60,389.    Elapsed: 0:05:06. Training loss. 0.00011663945770123973 Num fake examples 1736 Num true examples 1624\n",
      "  Batch 1,720  of  60,389.    Elapsed: 0:05:13. Training loss. 0.00013517416664399207 Num fake examples 1777 Num true examples 1663\n",
      "  Batch 1,760  of  60,389.    Elapsed: 0:05:20. Training loss. 0.00010668643517419696 Num fake examples 1815 Num true examples 1705\n",
      "  Batch 1,800  of  60,389.    Elapsed: 0:05:27. Training loss. 8.69295618031174e-05 Num fake examples 1856 Num true examples 1744\n",
      "  Batch 1,840  of  60,389.    Elapsed: 0:05:35. Training loss. 0.00012802239507436752 Num fake examples 1898 Num true examples 1782\n",
      "  Batch 1,880  of  60,389.    Elapsed: 0:05:42. Training loss. 8.964129665400833e-05 Num fake examples 1937 Num true examples 1823\n",
      "  Batch 1,920  of  60,389.    Elapsed: 0:05:49. Training loss. 9.148893877863884e-05 Num fake examples 1985 Num true examples 1855\n",
      "  Batch 1,960  of  60,389.    Elapsed: 0:05:56. Training loss. 0.00010406397632323205 Num fake examples 2023 Num true examples 1897\n",
      "  Batch 2,000  of  60,389.    Elapsed: 0:06:04. Training loss. 8.016500214580446e-05 Num fake examples 2064 Num true examples 1936\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:06:04\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.30\n",
      "  Validation Loss: 0.01\n",
      "  Validation took: 0:01:39\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:31:08 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_fake_examples = 0\n",
    "    total_true_examples = 0\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step > 2000:\n",
    "            break\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Training loss. {:} Num fake examples {:} Num true examples {:}'.format(step, len(train_dataloader), elapsed, train_loss,total_fake_examples, total_true_examples ))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        total_fake_examples += (b_labels == 1).sum().item()\n",
    "        total_true_examples += (b_labels == 0).sum().item()\n",
    "        #print (f\"{b_labels.shape=}\")\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "        #print (b_input_ids.shape, b_labels.shape, b_input_mask.shape, b_labels_one_hot.shape, b_labels_one_hot.dtype)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        train_loss= loss.item()\n",
    "        total_train_loss += train_loss\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        #print (f\"Training loss\", loss.item())\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    \n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        if step > 2000:\n",
    "            break\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "            output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels_one_hot)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    #Save model checkpoint\n",
    "    model.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6bb68-246e-4093-afb2-cf4363067b8a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "873821bc-1f66-44fe-acd0-7a312c358621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-9.3572,  9.3629]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def encode(sentence):\n",
    "    return tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 410,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "encoded_dict = encode(sentences[SENTENCE_INDEX])\n",
    "input_id = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "print (input_id.shape)\n",
    "model.eval()\n",
    "output = model(\n",
    "            input_id.cuda(),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=attention_mask.cuda(), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd7d7-aae7-4b04-afcc-664161c3e215",
   "metadata": {},
   "source": [
    "## Using validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7064487-72c4-43ba-a7ac-128221794554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a534156a-198b-495a-94b8-20b3e28f10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "model.eval()\n",
    "print (torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0).shape)\n",
    "output = model(torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(),dim=0), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca1659b2-4081-4161-b331-c0092badb90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.4784, -9.5297],\n",
      "        [ 9.4872, -9.5353]], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "tensor([[ 9.4721, -9.4860],\n",
      "        [ 9.4724, -9.5277]], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "tensor([[-9.0873,  9.0787],\n",
      "        [-9.5947,  9.6193]], device='cuda:0') tensor([1, 1], device='cuda:0')\n",
      "tensor([[ 9.4725, -9.5319],\n",
      "        [-9.5947,  9.6335]], device='cuda:0') tensor([0, 1], device='cuda:0')\n",
      "tensor([[-9.0775,  9.0677],\n",
      "        [-9.3149,  9.3171]], device='cuda:0') tensor([1, 1], device='cuda:0')\n",
      "tensor([[-9.5118,  9.5746],\n",
      "        [-9.5674,  9.6171]], device='cuda:0') tensor([1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(validation_dataloader):\n",
    "    if step > 5:\n",
    "        break\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(torch.int64).to(device)\n",
    "    b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        \n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        print (logits, b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f170c-91e6-4153-a3cf-872a6550bf69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
