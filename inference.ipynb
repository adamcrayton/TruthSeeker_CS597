{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17f45139-2436-400a-b1ae-4427b38b75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "DEVICE = torch.device('cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "CHECKPOINT_PATH = '/home/student/workspace/Truthseeker/checkpoints/checkpoint_with_maxlength_400_backup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f4f8872-2f81-4954-8114-eff337f18b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentence):\n",
    "    return tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 410,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bd776bf-07b1-4c91-87b8-0b35beded62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    CHECKPOINT_PATH, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "#Loading from statedict\n",
    "#model.load_state_dict(torch.load('final.ckpt'))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(DEVICE)\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed938d7f-3e88-4d5a-a8d8-22e8b9e8511e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATASET_PATH = \"/home/student/datasets/TruthSeeker2023/Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e330359d-317a-4252-96e5-7cc95defa761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']\n",
    "labels = df[\"BinaryNumTarget\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c97c772-b3c7-42a8-a4fd-68c8c79ee1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a251893-4acf-4f12-be3b-86ca15d44810",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "31e48200-4bb5-4b82-943e-e6eeaba52314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "torch.Size([1, 410])\n",
      "tensor([[-8.3545,  8.3569]]) tensor(0., dtype=torch.float64)\n",
      "torch.Size([1, 410])\n",
      "torch.Size([1, 410])\n",
      "tensor([[-8.3460,  8.3472]]) tensor(1., dtype=torch.float64)\n",
      "torch.Size([1, 410])\n",
      "torch.Size([1, 410])\n",
      "tensor([[-8.3609,  8.3624]]) tensor(0., dtype=torch.float64)\n",
      "torch.Size([1, 410])\n",
      "torch.Size([1, 410])\n",
      "tensor([[-8.3542,  8.3559]]) tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    encoded_sentence_dict = encode(sentences[i])\n",
    "    encoded_input_id = [encoded_sentence_dict['input_ids']]\n",
    "    encoded_attention_mask = [encoded_sentence_dict['attention_mask']]\n",
    "    concat_encoded_input_id = torch.cat(encoded_input_id, dim=0)\n",
    "    concat_attention_mask = torch.cat(encoded_attention_mask, dim=0)\n",
    "    dataset = TensorDataset(concat_encoded_input_id, concat_attention_mask)\n",
    "    input_id = torch.unsqueeze(dataset[0][0], dim=0)\n",
    "    input_mask = torch.unsqueeze(dataset[0][1], dim=0)\n",
    "    print (input_id.shape)\n",
    "    print (input_mask.shape)\n",
    "    with torch.no_grad(): \n",
    "        output = model(\n",
    "                input_id,\n",
    "                token_type_ids=None, \n",
    "                attention_mask=input_mask, return_dict=True)\n",
    "    print (output.logits, labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e6189-730a-4c99-970f-cc488a6cbbfc",
   "metadata": {},
   "source": [
    "# Inferencing as a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b7d67e2-23bc-405f-a61c-ee1312f668f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 1000/1000 [00:01<00:00, 566.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences[:1000]):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa0875b6-6aeb-40fe-87ec-d3161c713dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2706948/2028052492.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f83b5de-681d-4294-bc00-4009b519694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: @Riguy_453 @PaulSorrentino3 @POTUS Trump didn't attempt to mandate mask!! Biden wanted to more than the vaccine but you can't get Republican AMEricans to comply \n",
      "It was the Supreme Court that knocked the eviction moratorium on the head. The Amy Coney Barratts. Another Trump dirty trick.\n",
      "Token IDs: tensor([  101,  4861,  1024,  1000,  2017,  2113,  2054,  9733,  3825,  1999,\n",
      "         2976,  3318,  7773,  2197,  2095,  1029,  5717,  1012,  1000,  1064,\n",
      "         1056, 28394,  2102,  1024,  2049, 16498,  1010,  2073,  2017,  2123,\n",
      "         2102,  2707,  2012,  5717,  1012,  1996,  2769,  3310,  2013,  3316,\n",
      "         2066,  9733,  2040,  3825,  5717,  6363,  1999,  2976,  3318,  7773,\n",
      "         2197,  2095,  1012,  2049,  2256,  2769,  2000,  4088,  2007,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Labels: tensor(1., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "index = 15\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[index])\n",
    "print('Token IDs:', input_ids[index])\n",
    "print ('Labels:', labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4019c456-f8b6-47f2-a647-e2682bed5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a59b1784-15de-4e56-a689-cfeca8c2d490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7992e7fc-8399-474a-ba84-f1657171f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            dataset, # The validation samples.\n",
    "            sampler = RandomSampler(dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0f1251b-87f2-4198-afba-f544fb23310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.6795, -8.6905]], grad_fn=<AddmmBackward0>) tensor([0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "output = model(dataset[3:4][0], token_type_ids=None, attention_mask=dataset[3:4][1],labels=None)\n",
    "loss = output.loss\n",
    "logits = output.logits\n",
    "print (logits, dataset[3:4][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7885fac8-b7f0-49a3-852c-3af1b67c2074",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m (logits, \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mfor INDEX in range(1,20,1):\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m        print (logits, dataset[INDEX: INDEX+ 1][2])\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "for INDEX in range(1,20,1):\n",
    "    with torch.no_grad():\n",
    "        #output = model(dataset[INDEX:INDEX + 2][1], token_type_ids=None, attention_mask=dataset[INDEX:INDEX + 2][1],labels=None)\n",
    "        b_input_ids = dataset[INDEX:INDEX + 1][0]\n",
    "        attention_mask = dataset[INDEX:INDEX + 1][1]\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "            output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=None)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "        print (logits, dataset[INDEX: INDEX+ 1][2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6bdf1-bbaa-4848-9c54-deb8c0743253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011dc909-34d4-46ca-b051-f933d03eaa36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "08974419-a261-446c-b63a-a90c119515c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.5272,  8.5553],\n",
      "        [ 8.6726, -8.6745]]) tensor([1, 0])\n",
      "tensor([[-8.5503,  8.5755],\n",
      "        [-8.5136,  8.5264]]) tensor([1, 1])\n",
      "tensor([[ 8.6494, -8.6570],\n",
      "        [-8.5335,  8.5487]]) tensor([0, 1])\n",
      "tensor([[ 8.6742, -8.6542],\n",
      "        [ 8.6651, -8.6456]]) tensor([0, 0])\n",
      "tensor([[ 8.6865, -8.6853],\n",
      "        [-8.5263,  8.5356]]) tensor([0, 1])\n",
      "tensor([[-8.4601,  8.4752],\n",
      "        [ 8.6435, -8.6325]]) tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(validation_dataloader):\n",
    "    if step > 5:\n",
    "        break\n",
    "    b_input_ids = batch[0].to(DEVICE)\n",
    "    b_input_mask = batch[1].to(DEVICE)\n",
    "    b_labels = batch[2].to(torch.int64).to(DEVICE)\n",
    "    b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        \n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=None)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        print (logits, b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e91288bc-cdb3-4c6e-af10-3875b8db53dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357df11-4b8d-47c8-8e4e-27a27cbe4db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
