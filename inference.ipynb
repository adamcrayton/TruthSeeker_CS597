{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17f45139-2436-400a-b1ae-4427b38b75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "DEVICE = torch.device('cuda')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "CHECKPOINT_PATH = '/home/student/workspace/Truthseeker/final_checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f4f8872-2f81-4954-8114-eff337f18b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentence):\n",
    "    return tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bd776bf-07b1-4c91-87b8-0b35beded62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    CHECKPOINT_PATH, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "#Loading from statedict\n",
    "model.load_state_dict(torch.load('final.ckpt'))\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed938d7f-3e88-4d5a-a8d8-22e8b9e8511e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATASET_PATH = \"/home/student/datasets/TruthSeeker2023/Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e330359d-317a-4252-96e5-7cc95defa761",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']\n",
    "labels = df[\"BinaryNumTarget\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a251893-4acf-4f12-be3b-86ca15d44810",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31e48200-4bb5-4b82-943e-e6eeaba52314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/student/miniconda3/envs/truth_seeker/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.0100,  7.0272]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0134,  7.0322]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9947,  7.0076]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9979,  7.0151]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9918,  7.0070]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0144,  7.0324]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0112,  7.0294]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0027,  7.0198]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9759,  6.9822]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9938,  7.0016]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0153,  7.0297]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9939,  7.0099]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9972,  7.0131]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0088,  7.0265]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9828,  6.9922]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0019,  7.0187]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9962,  7.0114]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9871,  6.9974]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0022,  7.0182]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0013,  7.0160]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0096,  7.0242]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9964,  7.0115]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0870,  7.1000]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0144,  7.0315]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0152,  7.0308]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0054,  7.0190]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0039,  7.0246]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9992,  7.0123]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0260,  7.0458]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0311,  7.0469]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0126,  7.0329]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0008,  7.0186]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0179,  7.0372]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0111,  7.0300]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0207,  7.0394]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0076,  7.0242]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0118,  7.0247]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0127,  7.0290]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0128,  7.0274]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0103,  7.0297]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0077,  7.0169]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0076,  7.0259]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9990,  7.0104]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9710,  6.9752]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0014,  7.0154]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0093,  7.0237]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0011,  7.0102]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0114,  7.0241]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0060,  7.0196]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0161,  7.0343]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0142,  7.0320]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9969,  7.0050]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9991,  7.0134]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0047,  7.0159]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0011,  7.0142]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0005,  7.0056]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0065,  7.0227]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0158,  7.0374]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0136,  7.0326]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0130,  7.0297]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0121,  7.0287]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9997,  7.0130]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0157,  7.0358]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0045,  7.0201]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0136,  7.0291]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0095,  7.0285]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9890,  7.0001]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0046,  7.0226]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9784,  6.9838]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9875,  6.9969]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0125,  7.0247]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0143,  7.0306]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0063,  7.0246]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9947,  7.0090]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9916,  7.0032]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0122,  7.0249]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0055,  7.0214]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0188,  7.0374]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0123,  7.0286]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0066,  7.0244]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9930,  7.0045]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0078,  7.0239]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0075,  7.0252]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9993,  7.0156]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0065,  7.0204]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9997,  7.0138]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9962,  7.0099]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0084,  7.0183]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-6.9957,  7.0060]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-6.9963,  7.0112]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0167,  7.0311]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0049,  7.0227]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0028,  7.0165]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0046,  7.0201]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0137,  7.0272]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0166,  7.0307]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0203,  7.0345]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n",
      "tensor([[-7.0189,  7.0363]], device='cuda:0', grad_fn=<AddmmBackward0>) 0.0\n",
      "tensor([[-7.0105,  7.0260]], device='cuda:0', grad_fn=<AddmmBackward0>) 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100):\n",
    "    encoded_sentence_dict = encode(sentences[i])\n",
    "    #print (sentences[i])\n",
    "    #print (encoded_sentence_dict['input_ids'])\n",
    "    #print (tokenizer.decode(encoded_sentence_dict['input_ids'][0]))\n",
    "    #print (encoded_sentence_dict)\n",
    "    output = model(\n",
    "            encoded_sentence_dict['input_ids'].to(DEVICE),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=encoded_sentence_dict['attention_mask'].to(DEVICE), return_dict=True)\n",
    "    print (output.logits, labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c35adaa-acd7-4199-8eaf-caafd763039d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
